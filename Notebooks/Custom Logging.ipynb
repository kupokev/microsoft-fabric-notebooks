{"cells":[{"cell_type":"markdown","source":["# Custom Logging\n","This jupyter notebook is an example of how to implement custom logging in Microsoft Fabric using an Eventhouse. You can find my blog on this topic at [kevinoftech.com](https://www.kevinoftech.com/Blog/Post/2026-01-fabric-custom-logging)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"72e49fdc-5cc2-479e-84e3-1f89094bd793"},{"cell_type":"markdown","source":["## Import Python Libraries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64004d40-690d-4002-9ed9-e2e4f0e371b3"},{"cell_type":"code","source":["from datetime import datetime"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6286a7bc-72cb-4363-9760-70d5eaba13a2"},{"cell_type":"markdown","source":["## Setup Variables\n","#### logs\n","This is going to store all the logs in our notebook that will be written to the Eventhouse.\n","#### notebook_name\n","Using notebookutils.runtime.context we can get the name of the current notebook. This is helpful so that we don't have to update this variable in every new notesbook we copy this to. \n","#### workspace_name\n","Using notebookutils.runtime.context we can get the name of the current workspace. This is helpful so that we don't have to update this variable in every new notesbook we copy this to. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6bfe9447-f17f-4f4d-8dce-f4c5a8314511"},{"cell_type":"code","source":["logs = []\n","notebook_name = notebookutils.runtime.context.get(\"currentNotebookName\")\n","workspace_name = notebookutils.runtime.context.get(\"currentWorkspaceName\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6de2d317-0608-402a-be93-ba031b619e08"},{"cell_type":"markdown","source":["## Log Function\n","We will use this function to add logs to the log variable we set above. \n","\n","Notice that the log_trace parameter is optional. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ec615f8-e272-4d22-9615-82a30da93d00"},{"cell_type":"code","source":["def log(log_level, log_message, log_trace=None):\n","    logs.append({\n","        \"log_datetime\": datetime.now(), \n","        \"workspace_name\": workspace_name,\n","        \"notebook_name\": notebook_name,\n","        \"log_level\": log_level,\n","        \"log_message\": log_message,\n","        \"log_trace\": log_trace\n","    })"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8823f074-06e4-4e39-9844-13af94a4ebab"},{"cell_type":"markdown","source":["## Write to Eventhouse Function\n","This function is called at the end of the notebook to actually write the logs to the Eventhouse. \n","\n","#### Note 1\n","Make sure all the variables in this code block are set correctly.\n","- database\n","- table\n","- eventhouse_uri\n","- columns_in_order\n","\n","#### Note 2\n","Notice the array \"columns_in_order\". This has to be set for when we call our spark.createDataFrame() we can set the order of the columns in the dataframe. If we do not do this the data will be sent to the Eventhouse in the wrong columns."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0a1bb0f8-9f3a-4c82-990b-eb4cb0e6e13a"},{"cell_type":"code","source":["def write_to_eventhouse(logs):\n","    database = \"Logging\"\n","    table = \"NotebookLogging\"\n","    eventhouse_uri = \"https://<your-kusto-cluster>.kusto.fabric.microsoft.com\"\n","        \n","    columns_in_order = [\n","        \"log_datetime\", \n","        \"workspace_name\",\n","        \"notebook_name\", \n","        \"log_level\", \n","        \"log_message\", \n","        \"log_trace\"\n","    ]\n","\n","    try:\n","        df = spark.createDataFrame(logs).select(columns_in_order) # .select() puts the columns in the correct order\n","        \n","        df.write.format(\"com.microsoft.kusto.spark.synapse.datasource\").\\\n","        option(\"kustoCluster\", eventhouse_uri).\\\n","        option(\"kustoDatabase\", database).\\\n","        option(\"kustoTable\", table).\\\n","        option(\"accessToken\", mssparkutils.credentials.getToken(eventhouse_uri)).\\\n","        option(\"tableCreateOptions\", \"CreateIfNotExist\").mode(\"Append\").save()\n","    except Exception as e:\n","        print(e)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f079058-5cd6-451b-9518-d98b559d29c0"},{"cell_type":"markdown","source":["## Test Creating Logs\n","The log() function has an optional parameter called \"log_trace\".\n","#### Test 1: No Trace\n","In this example, we leave the \"log_trace\" parameter blank since there is nothing to trace. \n","#### Test 2: With trace\n","In this example, we trigger an error and capture it so we can see that a trace would look like in the logs. It should look something like this\n","\n","```\n","Traceback (most recent call last):\\n  File \\\"/tmp/ipykernel_5632/1710815062.py\\\", line 8, in <module>\\n    1 / 0  # This raises a ZeroDivisionError\\n    ~~^~~\\nZeroDivisionError: division by zero\\n\n","```\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be1b5f44-c228-433d-a41b-17628e89a819"},{"cell_type":"code","source":["# Test 1: No Trace\n","log(\"Warning\", \"Test message 1\")\n","\n","# Test 2: With trace\n","import traceback\n","\n","try:\n","    1 / 0  # This raises a ZeroDivisionError\n","except Exception as e:\n","    log(\"Warning\", \"Test message 2\", traceback.format_exc())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af5cbac8-afe2-40be-af89-7c5c4a681b9e"},{"cell_type":"markdown","source":["## Write the Logs to the Eventhouse\n","This will write the content of logs array to the Eventhouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cde8e8a1-9f93-4751-845c-a905b2b887ff"},{"cell_type":"code","source":["write_to_eventhouse(logs)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1d60372-1b8d-47be-a260-9040de21e7e1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}