{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1c680-93d4-4126-84cf-793ac5d6f0bc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e38f61-780e-49cf-a57c-94a228c7d071",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91ffe1-7b14-4544-81fb-f9fc6e1adb5c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def get_table_list():\n",
    "    # Get default lakehouse name\n",
    "    lakehouse_name = notebookutils.runtime.context.get(\"defaultLakehouseName\")\n",
    "\n",
    "    # Get list of tables from lakehouse\n",
    "    tables = notebookutils.lakehouse.listTables(lakehouse_name)\n",
    "\n",
    "    return [table.name for table in tables] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f76e6-e058-43a3-b332-c70096a43bf1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def refresh_table(table_name):\n",
    "    try:\n",
    "        print(f\"[{datetime.now()}] Starting refresh for table: {table_name}\")\n",
    "        \n",
    "        spark.sql(f\"OPTIMIZE `{table_name}`\")\n",
    "        spark.sql(f\"VACUUM `{table_name}` RETAIN 168 HOURS\")\n",
    "\n",
    "        spark.sql(f\"REFRESH TABLE `{table_name}`\")\n",
    "        spark.sql(f\"ANALYZE TABLE `{table_name}` COMPUTE STATISTICS\")\n",
    "        \n",
    "        print(f\"[{datetime.now()}] Completed refresh for table: {table_name}\")\n",
    "        return f\"Success: {table_name}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Failed to refresh {table_name}: {str(e)}\"\n",
    "        print(f\"[{datetime.now()}] {error_msg}\")\n",
    "        return f\"Error: {error_msg}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ff5bd-c1b5-4a70-9482-ac7921546884",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Set Variables To Be Used\n",
    "\n",
    "1. max_workers - Adjust this based on your Fabric capacity or Notebook Environment limits\n",
    "2. timeout_limit - Max time (seconds) limit a worker is allowed to run \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233f820-a6d1-4138-b403-ff70afc1ec97",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "max_workers = 5\n",
    "timeout_limit = 300\n",
    "table_list = {\n",
    "    \"fake_erp_dbo_customer\",\n",
    "    \"fake_erp_dbo_purchase_order\"\n",
    "}\n",
    "\n",
    "# Alternative approach is get ALL the tables in the lakehouse\n",
    "# table_list = get_table_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2451d0-bd41-4ef2-b85f-181e8933a20d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Process Tables Concurrently\n",
    "\n",
    "For more information on concurrent.futures visit https://docs.python.org/3/library/concurrent.futures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5c0e3-f57b-4b55-98ea-0f33a6867222",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "job_start_time = datetime.now()\n",
    "print(f\"[{job_start_time}] Job Starting\")\n",
    "    \n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_table = {\n",
    "        executor.submit(refresh_table, table): table for table in table_list\n",
    "    }\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(future_to_table):\n",
    "        table_name = future_to_table[future]\n",
    "        try:\n",
    "            result = future.result(timeout=timeout_limit)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            error_msg = f\"Timeout: {table_name} took longer than {timeout_limit} seconds\"\n",
    "            print(f\"[{datetime.now()}] {error_msg}\")\n",
    "        except Exception as exc:\n",
    "            error_msg = f\"Exception for {table_name}: {exc}\"\n",
    "            print(f\"[{datetime.now()}] {error_msg}\")\n",
    "\n",
    "job_end_time = datetime.now()\n",
    "job_duration = (job_end_time - job_start_time).total_seconds()\n",
    "\n",
    "print(f\"[{datetime.now()}] Job Completed in {job_duration:.1f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "522f9d43-eedc-4d1f-8dbc-5cceb4228818",
    "default_lakehouse_name": "lab_lakehouse",
    "default_lakehouse_workspace_id": "1ccd4a8b-bfc2-49b8-85f8-f202631243ef",
    "known_lakehouses": [
     {
      "id": "522f9d43-eedc-4d1f-8dbc-5cceb4228818"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
